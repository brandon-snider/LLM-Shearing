{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandon/Documents/College/q4-fall-24/cs-229/project/LLM-Shearing/.env/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-14m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandon/Documents/College/q4-fall-24/cs-229/project/LLM-Shearing/.env/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tok_llama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", token=\"<TOKEN>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False)}, clean_up_tokenization_spaces=False)\n"
     ]
    }
   ],
   "source": [
    "print(tok_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36e35bea12e4b39809d2454860f6dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408142531680421dabf02f847820c78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "\n",
    "ds = load_dataset(\n",
    "    # \"mlfoundations/dclm-baseline-1.0\",\n",
    "    \"bigcode/the-stack-v2-train-smol-ids\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")\n",
    "# ds = ds.shuffle(seed=42, buffer_size=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d339a0d15ba94425bb47a3338275dd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2e6fbcad874a53979642f5306901ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import data_helper\n",
      "import time\n",
      "import datetime\n",
      "import os\n",
      "import tensorflow as tf\n",
      "\n",
      "import numpy as np\n",
      "import evaluation\n",
      "now = int(time.time()) \n",
      "    \n",
      "timeArray = time.localtime(now)\n",
      "timeStamp = time.strftime(\"%Y%m%d%H%M%S\", timeArray)\n",
      "timeDay = time.strftime(\"%Y%m%d\", timeArray)\n",
      "print (timeStamp)\n",
      "\n",
      "def main(args):\n",
      "    args._parse_flags()\n",
      "    print(\"\\nParameters:\")\n",
      "    for attr, value in sorted(args.__flags.items()):\n",
      "        print((\"{}={}\".format(attr.upper(), value)))\n",
      "    log_dir = 'log/'+ timeDay\n",
      "    if not os.path.exists(log_dir):\n",
      "        os.makedirs(log_dir)\n",
      "    data_file = log_dir + '/test_' + args.data + timeStamp\n",
      "    precision = data_file + 'precise'\n",
      "    print('load data ...........')\n",
      "    train,test,dev = data_helper.load(args.data,filter = args.clean)\n",
      "\n",
      "    q_max_sent_length = max(map(lambda x:len(x),train['question'].str.split()))\n",
      "    a_max_sent_length = max(map(lambda x:len(x),train['answer'].str.split()))\n",
      "\n",
      "    alphabet = data_helper.get_alphabet([train,test,dev])\n",
      "    print('the number of words',len(alphabet))\n",
      "\n",
      "    print('get embedding')\n",
      "    if args.data==\"quora\":\n",
      "        embedding = data_helper.get_embedding(alphabet,language=\"cn\")\n",
      "    else:\n",
      "        embedding = data_helper.get_embedding(alphabet)\n",
      "    \n",
      "    \n",
      "\n",
      "    with tf.Graph().as_default(), tf.device(\"/gpu:\" + str(args.gpu)):\n",
      "        # with tf.device(\"/cpu:0\"):\n",
      "        session_conf = tf.ConfigProto()\n",
      "        session_conf.allow_soft_placement = args.allow_soft_placement\n",
      "        session_conf.log_device_placement = args.log_device_placement\n",
      "        session_conf.gpu_options.allow_growth = True\n",
      "        sess = tf.Session(config=session_conf)\n",
      "\n",
      "        model = QA_CNN_extend(max_input_left = q_max_sent_length,\n",
      "            max_input_right = a_max_sent_length,\n",
      "            batch_size = args.batch_size,\n",
      "            vocab_size = len(alphabet),\n",
      "            embedding_size = args.embedding_dim,\n",
      "            filter_sizes = list(map(int, args.filter_sizes.split(\",\"))),\n",
      "            num_filters = args.num_filters, \n",
      "            hidden_size = args.hidden_size,\n",
      "            dropout_keep_prob = args.dropout_keep_prob,\n",
      "            embeddings = embedding,\n",
      "            l2_reg_lambda = args.l2_reg_lambda,\n",
      "            trainable = args.trainable,\n",
      "            pooling = args.pooling,\n",
      "            conv = args.conv)\n",
      "\n",
      "        model.build_graph()\n",
      "\n",
      "        sess.run(tf.global_variables_initializer())\n",
      "        def train_step(model,sess,batch):\n",
      "            for data in batch:\n",
      "                feed_dict = {\n",
      "                    model.question:data[0],\n",
      "                    model.answer:data[1],\n",
      "                    model.answer_negative:data[2],\n",
      "                    model.q_mask:data[3],\n",
      "                    model.a_mask:data[4],\n",
      "                    model.a_neg_mask:data[5]\n",
      "\n",
      "                }\n",
      "                _, summary, step, loss, accuracy,score12, score13, see = sess.run(\n",
      "                        [model.train_op, model.merged,model.global_step,model.loss, model.accuracy,model.score12,model.score13, model.see],\n",
      "                        feed_dict)\n",
      "                time_str = datetime.datetime.now().isoformat()\n",
      "                print(\"{}: step {}, loss {:g}, acc {:g} ,positive {:g},negative {:g}\".format(time_str, step, loss, accuracy,np.mean(score12),np.mean(score13)))\n",
      "        def predict(model,sess,batch,test):\n",
      "            scores = []\n",
      "            for data in batch:\n",
      "                feed_dict = {\n",
      "                    model.question:data[0],\n",
      "                    model.answer:data[1],\n",
      "                    model.q_mask:data[2],\n",
      "                    model.a_mask:data[3]\n",
      "\n",
      "                }\n",
      "                score = sess.run(\n",
      "                        model.score12,\n",
      "                        feed_dict)\n",
      "                scores.extend(score)\n",
      "      \n",
      "            return np.array(scores[:len(test)])\n",
      "        \n",
      "                \n",
      "        \n",
      "\n",
      "        \n",
      "        for i in range(args.num_epoches):\n",
      "            datas = data_helper.get_mini_batch(train,alphabet,args.batch_size)\n",
      "            train_step(model,sess,datas)\n",
      "            test_datas = data_helper.get_mini_batch_test(test,alphabet,args.batch_size)\n",
      "\n",
      "            predicted_test = predict(model,sess,test_datas,test)\n",
      "            print(len(predicted_test))\n",
      "            print(len(test))\n",
      "            map_mrr_test = evaluation.evaluationBypandas(test,predicted_test)\n",
      "\n",
      "            print('map_mrr test',map_mrr_test)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "class Singleton(object):\n",
      "    __instance=None\n",
      "    def __init__(self):\n",
      "        pass\n",
      "    def getInstance(self):\n",
      "        if Singleton.__instance is None:\n",
      "            # Singleton.__instance=object.__new__(cls,*args,**kwd)\n",
      "            Singleton.__instance=self.get_test_flag()\n",
      "            print(\"build FLAGS over\")\n",
      "        return Singleton.__instance\n",
      "    def get_test_flag(self):\n",
      "        import tensorflow as tf\n",
      "        flags = tf.app.flags\n",
      "        if len(flags.FLAGS.__dict__.keys())<=2:\n",
      "\n",
      "            flags.DEFINE_integer(\"embedding_size\",300, \"Dimensionality of character embedding (default: 128)\")\n",
      "            flags.DEFINE_string(\"filter_sizes\", \"1,2,3,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
      "            flags.DEFINE_integer(\"num_filters\", 64, \"Number of filters per filter size (default: 128)\")\n",
      "            flags.DEFINE_float(\"dropout_keep_prob\", 1, \"Dropout keep probability (default: 0.5)\")\n",
      "            flags.DEFINE_float(\"l2_reg_lambda\", 0.000001, \"L2 regularizaion lambda (default: 0.0)\")\n",
      "            flags.DEFINE_float(\"learning_rate\", 5e-3, \"learn rate( default: 0.0)\")\n",
      "            flags.DEFINE_integer(\"max_len_left\", 40, \"max document length of left input\")\n",
      "            flags.DEFINE_integer(\"max_len_right\", 40, \"max document length of right input\")\n",
      "            flags.DEFINE_string(\"loss\",\"pair_wise\",\"loss function (default:point_wise)\")\n",
      "            flags.DEFINE_integer(\"hidden_size\",100,\"the default hidden size\")\n",
      "            flags.DEFINE_string(\"model_name\", \"cnn\", \"cnn or rnn\")\n",
      "\n",
      "            # Training parameters\n",
      "            flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
      "            flags.DEFINE_boolean(\"trainable\", False, \"is embedding trainable? (default: False)\")\n",
      "            flags.DEFINE_integer(\"num_epoches\", 1000, \"Number of training epochs (default: 200)\")\n",
      "            flags.DEFINE_integer(\"evaluate_every\", 500, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
      "            flags.DEFINE_integer(\"checkpoint_every\", 500, \"Save model after this many steps (default: 100)\")\n",
      "\n",
      "            flags.DEFINE_string('data','wiki','data set')\n",
      "            flags.DEFINE_string('pooling','max','max pooling or attentive pooling')\n",
      "            flags.DEFINE_boolean('clean',True,'whether we clean the data')\n",
      "            flags.DEFINE_string('conv','wide','wide conv or narrow')\n",
      "            flags.DEFINE_integer('gpu',0,'gpu number')\n",
      "            # Misc Parameters\n",
      "            flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
      "            flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
      "        return flags.FLAGS\n",
      "    def get_rnn_flag(self):\n",
      "        import tensorflow as tf\n",
      "        flags = tf.app.flags\n",
      "        if len(flags.FLAGS.__dict__.keys())<=2:\n",
      "\n",
      "            flags.DEFINE_integer(\"embedding_size\",300, \"Dimensionality of character embedding (default: 128)\")\n",
      "            flags.DEFINE_string(\"filter_sizes\", \"1,2,3,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
      "            flags.DEFINE_integer(\"num_filters\", 64, \"Number of filters per filter size (default: 128)\")\n",
      "            flags.DEFINE_float(\"dropout_keep_prob\", 1, \"Dropout keep probability (default: 0.5)\")\n",
      "            flags.DEFINE_float(\"l2_reg_lambda\", 0.000001, \"L2 regularizaion lambda (default: 0.0)\")\n",
      "            flags.DEFINE_float(\"learning_rate\", 0.001, \"learn rate( default: 0.0)\")\n",
      "            flags.DEFINE_integer(\"max_len_left\", 40, \"max document length of left input\")\n",
      "            flags.DEFINE_integer(\"max_len_right\", 40, \"max document length of right input\")\n",
      "            flags.DEFINE_string(\"loss\",\"pair_wise\",\"loss function (default:point_wise)\")\n",
      "            flags.DEFINE_integer(\"hidden_size\",100,\"the default hidden size\")\n",
      "            flags.DEFINE_string(\"model_name\", \"rnn\", \"cnn or rnn\")\n",
      "\n",
      "            # Training parameters\n",
      "            flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
      "            flags.DEFINE_boolean(\"trainable\", False, \"is embedding trainable? (default: False)\")\n",
      "            flags.DEFINE_integer(\"num_epoches\", 1000, \"Number of training epochs (default: 200)\")\n",
      "            flags.DEFINE_integer(\"evaluate_every\", 500, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
      "            flags.DEFINE_integer(\"checkpoint_every\", 500, \"Save model after this many steps (default: 100)\")\n",
      "\n",
      "\n",
      "#            flags.DEFINE_string('data','8008','data set')\n",
      "\n",
      "            flags.DEFINE_string('data','trec','data set')\n",
      "\n",
      "            flags.DEFINE_string('pooling','max','max pooling or attentive pooling')\n",
      "            flags.DEFINE_boolean('clean',False,'whether we clean the data')\n",
      "            flags.DEFINE_string('conv','wide','wide conv or narrow')\n",
      "            flags.DEFINE_integer('gpu',0,'gpu number')\n",
      "            # Misc Parameters\n",
      "            flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
      "            flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
      "        return flags.FLAGS\n",
      "    def get_cnn_flag(self):\n",
      "        import tensorflow as tf\n",
      "        flags = tf.app.flags\n",
      "        if len(flags.FLAGS.__dict__.keys())<=2:\n",
      "\n",
      "            flags.DEFINE_integer(\"embedding_size\",300, \"Dimensionality of character embedding (default: 128)\")\n",
      "            flags.DEFINE_string(\"filter_sizes\", \"1,2,3,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
      "            flags.DEFINE_integer(\"num_filters\", 64, \"Number of filters per filter size (default: 128)\")\n",
      "            flags.DEFINE_float(\"dropout_keep_prob\", 0.8, \"Dropout keep probability (default: 0.5)\")\n",
      "            flags.DEFINE_float(\"l2_reg_lambda\", 0.000001, \"L2 regularizaion lambda (default: 0.0)\")\n",
      "            flags.DEFINE_float(\"learning_rate\", 5e-3, \"learn rate( default: 0.0)\")\n",
      "            flags.DEFINE_integer(\"max_len_left\", 40, \"max document length of left input\")\n",
      "            flags.DEFINE_integer(\"max_len_right\", 40, \"max document length of right input\")\n",
      "            flags.DEFINE_string(\"loss\",\"pair_wise\",\"loss function (default:point_wise)\")\n",
      "            flags.DEFINE_integer(\"hidden_size\",100,\"the default hidden size\")\n",
      "            flags.DEFINE_string(\"model_name\", \"cnn\", \"cnn or rnn\")\n",
      "\n",
      "            # Training parameters\n",
      "            flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
      "            flags.DEFINE_boolean(\"trainable\", False, \"is embedding trainable? (default: False)\")\n",
      "            flags.DEFINE_integer(\"num_epoches\", 1000, \"Number of training epochs (default: 200)\")\n",
      "            flags.DEFINE_integer(\"evaluate_every\", 500, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
      "            flags.DEFINE_integer(\"checkpoint_every\", 500, \"Save model after this many steps (default: 100)\")\n",
      "\n",
      "            flags.DEFINE_string('data','wiki','data set')\n",
      "            flags.DEFINE_string('pooling','max','max pooling or attentive pooling')\n",
      "            flags.DEFINE_boolean('clean',True,'whether we clean the data')\n",
      "            flags.DEFINE_string('conv','wide','wide conv or narrow')\n",
      "            flags.DEFINE_integer('gpu',0,'gpu number')\n",
      "            # Misc Parameters\n",
      "            flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
      "            flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
      "        return flags.FLAGS\n",
      "\n",
      "\n",
      "    def get_qcnn_flag(self):\n",
      "\n",
      "        import tensorflow as tf\n",
      "        flags = tf.app.flags\n",
      "        if len(flags.FLAGS.__dict__.keys())<=2:\n",
      "\n",
      "\n",
      "            flags.DEFINE_integer(\"embedding_size\",300, \"Dimensionality of character embedding (default: 128)\")\n",
      "            flags.DEFINE_string(\"filter_sizes\", \"1,2,3,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
      "            flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
      "            flags.DEFINE_float(\"dropout_keep_prob\", 0.8, \"Dropout keep probability (default: 0.5)\")\n",
      "            flags.DEFINE_float(\"l2_reg_lambda\", 0.000001, \"L2 regularizaion lambda (default: 0.0)\")\n",
      "            flags.DEFINE_float(\"learning_rate\", 0.001, \"learn rate( default: 0.0)\")\n",
      "\n",
      "            flags.DEFINE_integer(\"max_len_left\", 40, \"max document length of left input\")\n",
      "            flags.DEFINE_integer(\"max_len_right\", 40, \"max document length of right input\")\n",
      "            flags.DEFINE_string(\"loss\",\"pair_wise\",\"loss function (default:point_wise)\")\n",
      "            flags.DEFINE_integer(\"hidden_size\",100,\"the default hidden size\")\n",
      "\n",
      "            flags.DEFINE_string(\"model_name\", \"qcnn\", \"cnn or rnn\")\n",
      "\n",
      "\n",
      "            # Training parameters\n",
      "            flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
      "            flags.DEFINE_boolean(\"trainable\", False, \"is embedding trainable? (default: False)\")\n",
      "            flags.DEFINE_integer(\"num_epoches\", 1000, \"Number of training epochs (default: 200)\")\n",
      "            flags.DEFINE_integer(\"evaluate_every\", 500, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
      "            flags.DEFINE_integer(\"checkpoint_every\", 500, \"Save model after this many steps (default: 100)\")\n",
      "\n",
      "\n",
      "            flags.DEFINE_string('data','wiki','data set')\n",
      "            flags.DEFINE_string('pooling','mean','max pooling or attentive pooling')\n",
      "\n",
      "            flags.DEFINE_boolean('clean',True,'whether we clean the data')\n",
      "            flags.DEFINE_string('conv','wide','wide conv or narrow')\n",
      "            flags.DEFINE_integer('gpu',0,'gpu number')\n",
      "            # Misc Parameters\n",
      "            flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
      "            flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
      "        return flags.FLAGS\n",
      "\n",
      "    def get_8008_flag(self):\n",
      "        import tensorflow as tf\n",
      "        flags = tf.app.flags\n",
      "        if len(flags.FLAGS.__dict__.keys())<=2:\n",
      "\n",
      "            flags.DEFINE_integer(\"embedding_size\",200, \"Dimensionality of character embedding (default: 128)\")\n",
      "            flags.DEFINE_string(\"filter_sizes\", \"1,2,3,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
      "            flags.DEFINE_integer(\"num_filters\", 64, \"Number of filters per filter size (default: 128)\")\n",
      "            flags.DEFINE_float(\"dropout_keep_prob\", 0.8, \"Dropout keep probability (default: 0.5)\")\n",
      "            flags.DEFINE_float(\"l2_reg_lambda\", 0.000001, \"L2 regularizaion lambda (default: 0.0)\")\n",
      "            flags.DEFINE_float(\"learning_rate\", 1e-3, \"learn rate( default: 0.0)\")\n",
      "            flags.DEFINE_integer(\"max_len_left\", 40, \"max document length of left input\")\n",
      "            flags.DEFINE_integer(\"max_len_right\", 40, \"max document length of right input\")\n",
      "            flags.DEFINE_string(\"loss\",\"pair_wise\",\"loss function (default:point_wise)\")\n",
      "            flags.DEFINE_integer(\"hidden_size\",100,\"the default hidden size\")\n",
      "            flags.DEFINE_string(\"model_name\", \"rnn\", \"cnn or rnn\")\n",
      "\n",
      "            # Training parameters\n",
      "            flags.DEFINE_integer(\"batch_size\", 250, \"Batch Size (default: 64)\")\n",
      "            flags.DEFINE_boolean(\"trainable\", False, \"is embedding trainable? (default: False)\")\n",
      "            flags.DEFINE_integer(\"num_epoches\", 1000, \"Number of training epochs (default: 200)\")\n",
      "            flags.DEFINE_integer(\"evaluate_every\", 500, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
      "            flags.DEFINE_integer(\"checkpoint_every\", 500, \"Save model after this many steps (default: 100)\")\n",
      "\n",
      "            flags.DEFINE_string('data','8008','data set')\n",
      "            flags.DEFINE_string('pooling','max','max pooling or attentive pooling')\n",
      "            flags.DEFINE_boolean('clean',False,'whether we clean the data')\n",
      "            flags.DEFINE_string('conv','wide','wide conv or narrow')\n",
      "            flags.DEFINE_integer('gpu',0,'gpu number')\n",
      "            # Misc Parameters\n",
      "            flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
      "            flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
      "        return flags.FLAGS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "if __name__==\"__main__\":\n",
      "    args=Singleton().get_test_flag()\n",
      "    for attr, value in sorted(args.__flags.items()):\n",
      "        print((\"{}={}\".format(attr.upper(), value)))\n",
      "   \n",
      "from tensorflow import flags\n",
      "import tensorflow as tf\n",
      "from config import Singleton\n",
      "import data_helper\n",
      "\n",
      "import datetime,os\n",
      "\n",
      "import models\n",
      "import numpy as np\n",
      "import evaluation\n",
      "\n",
      "import sys\n",
      "import logging\n",
      "\n",
      "import time\n",
      "now = int(time.time())\n",
      "timeArray = time.localtime(now)\n",
      "timeStamp = time.strftime(\"%Y%m%d%H%M%S\", timeArray)\n",
      "log_filename = \"log/\" +time.strftime(\"%Y%m%d\", timeArray)\n",
      "\n",
      "program = os.path.basename('program')\n",
      "logger = logging.getLogger(program) \n",
      "if not os.path.exists(log_filename):\n",
      "    os.makedirs(log_filename)\n",
      "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s',datefmt='%a, %d %b %Y %H:%M:%S',filename=log_filename+'/qa.log',filemode='w')\n",
      "logging.root.setLevel(level=logging.INFO)\n",
      "logger.info(\"running %s\" % ' '.join(sys.argv))\n",
      "    \n",
      "\n",
      "\n",
      "from data_helper import log_time_delta,getLogger\n",
      "\n",
      "logger=getLogger()\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "args = Singleton().get_qcnn_flag()\n",
      "\n",
      "args._parse_flags()\n",
      "opts=dict()\n",
      "logger.info(\"\\nParameters:\")\n",
      "for attr, value in sorted(args.__flags.items()):\n",
      "    logger.info((\"{}={}\".format(attr.upper(), value)))\n",
      "    opts[attr]=value\n",
      "\n",
      "\n",
      "train,test,dev = data_helper.load(args.data,filter = args.clean)\n",
      "\n",
      "q_max_sent_length = max(map(lambda x:len(x),train['question'].str.split()))\n",
      "a_max_sent_length = max(map(lambda x:len(x),train['answer'].str.split()))\n",
      "\n",
      "alphabet = data_helper.get_alphabet([train,test,dev],dataset=args.data )\n",
      "logger.info('the number of words :%d '%len(alphabet))\n",
      "\n",
      "if args.data==\"quora\" or  args.data==\"8008\" :\n",
      "    print(\"cn embedding\")\n",
      "    embedding = data_helper.get_embedding(alphabet,dim=200,language=\"cn\",dataset=args.data )\n",
      "    train_data_loader = data_helper.getBatch48008\n",
      "else:\n",
      "    embedding = data_helper.get_embedding(alphabet,dim=300,dataset=args.data )\n",
      "    train_data_loader = data_helper.get_mini_batch\n",
      "opts[\"embeddings\"] =embedding\n",
      "opts[\"vocab_size\"]=len(alphabet)\n",
      "opts[\"max_input_right\"]=a_max_sent_length\n",
      "opts[\"max_input_left\"]=q_max_sent_length\n",
      "opts[\"filter_sizes\"]=list(map(int, args.filter_sizes.split(\",\")))\n",
      "\n",
      "print(\"innitilize over\")\n",
      "\n",
      "\n",
      "   \n",
      " \n",
      "#with tf.Graph().as_default(), tf.device(\"/gpu:\" + str(args.gpu)):\n",
      "with tf.Graph().as_default():    \n",
      "    # with tf.device(\"/cpu:0\"):\n",
      "    session_conf = tf.ConfigProto()\n",
      "    session_conf.allow_soft_placement = args.allow_soft_placement\n",
      "    session_conf.log_device_placement = args.log_device_placement\n",
      "    session_conf.gpu_options.allow_growth = True\n",
      "    sess = tf.Session(config=session_conf)\n",
      "    model=models.setup(opts)\n",
      "    model.build_graph()    \n",
      "    saver = tf.train.Saver()\n",
      "    \n",
      "#    ckpt = tf.train.get_checkpoint_state(\"checkpoint\")    \n",
      "#    if ckpt and ckpt.model_checkpoint_path:    \n",
      "#        # Restores from checkpoint    \n",
      "#        saver.restore(sess, ckpt.model_checkpoint_path)\n",
      "#    if os.path.exists(\"model\") :                        \n",
      "#        import shutil\n",
      "#        shutil.rmtree(\"model\")        \n",
      "#    builder = tf.saved_model.builder.SavedModelBuilder(\"./model\")\n",
      "#    builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING])\n",
      "#    builder.save(True)\n",
      "#    variable_averages = tf.train.ExponentialMovingAverage(  model)    \n",
      "#    variables_to_restore = variable_averages.variables_to_restore()    \n",
      "#    saver = tf.train.Saver(variables_to_restore)  \n",
      "#    for name in variables_to_restore:    \n",
      "#        print(name) \n",
      "\n",
      "    sess.run(tf.global_variables_initializer())\n",
      "    @log_time_delta\n",
      "    def predict(model,sess,batch,test):\n",
      "        scores = []\n",
      "        for data in batch:            \n",
      "            score = model.predict(sess,data)\n",
      "            scores.extend(score)  \n",
      "        return np.array(scores[:len(test)])\n",
      "    \n",
      "    best_p1=0\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    for i in range(args.num_epoches):  \n",
      "        \n",
      "        for data in train_data_loader(train,alphabet,args.batch_size,model=model,sess=sess):\n",
      "#        for data in data_helper.getBatch48008(train,alphabet,args.batch_size):\n",
      "            _, summary, step, loss, accuracy,score12, score13, see = model.train(sess,data)\n",
      "            time_str = datetime.datetime.now().isoformat()\n",
      "            print(\"{}: step {}, loss {:g}, acc {:g} ,positive {:g},negative {:g}\".format(time_str, step, loss, accuracy,np.mean(score12),np.mean(score13)))\n",
      "            logger.info(\"{}: step {}, loss {:g}, acc {:g} ,positive {:g},negative {:g}\".format(time_str, step, loss, accuracy,np.mean(score12),np.mean(score13)))\n",
      "#<<<<<<< HEAD\n",
      "#        \n",
      "# \n",
      "#        if i>0 and i % 5 ==0:\n",
      "#            test_datas = data_helper.get_mini_batch_test(test,alphabet,args.batch_size)\n",
      "#        \n",
      "#            predicted_test = predict(model,sess,test_datas,test)\n",
      "#            map_mrr_test = evaluation.evaluationBypandas(test,predicted_test)\n",
      "#        \n",
      "#            logger.info('map_mrr test' +str(map_mrr_test))\n",
      "#            print('map_mrr test' +str(map_mrr_test))\n",
      "#            \n",
      "#            test_datas = data_helper.get_mini_batch_test(dev,alphabet,args.batch_size)\n",
      "#            predicted_test = predict(model,sess,test_datas,dev)\n",
      "#            map_mrr_test = evaluation.evaluationBypandas(dev,predicted_test)\n",
      "#        \n",
      "#            logger.info('map_mrr dev' +str(map_mrr_test))\n",
      "#            print('map_mrr dev' +str(map_mrr_test))\n",
      "#            map,mrr,p1 = map_mrr_test\n",
      "#            if p1>best_p1:\n",
      "#                best_p1=p1\n",
      "#                filename= \"checkpoint/\"+args.data+\"_\"+str(p1)+\".model\"\n",
      "#                save_path = saver.save(sess, filename)  \n",
      "#        #            load_path = saver.restore(sess, model_path)\n",
      "#                \n",
      "#                import shutil\n",
      "#                shutil.rmtree(\"model\")\n",
      "#                builder = tf.saved_model.builder.SavedModelBuilder(\"./model\")\n",
      "#                builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING])\n",
      "#                builder.save(True)\n",
      "#        \n",
      "#        \n",
      "#=======\n",
      "\n",
      "        test_datas = data_helper.get_mini_batch_test(test,alphabet,args.batch_size)\n",
      "\n",
      "        predicted_test = predict(model,sess,test_datas,test)\n",
      "        map_mrr_test = evaluation.evaluationBypandas(test,predicted_test)\n",
      "\n",
      "        logger.info('map_mrr test' +str(map_mrr_test))\n",
      "        print('epoch '+ str(i) + 'map_mrr test' +str(map_mrr_test))\n",
      "\n",
      "\n",
      "#coding:utf-8\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "from tensorflow.contrib import rnn\n",
      "import models.blocks as blocks\n",
      "# model_type :apn or qacnn\n",
      "class QA_CNN_extend(object):\n",
      "#    def __init__(self,max_input_left,max_input_right,batch_size,vocab_size,embedding_size,filter_sizes,num_filters,hidden_size,\n",
      "#        dropout_keep_prob = 1,learning_rate = 0.001,embeddings = None,l2_reg_lambda = 0.0,trainable = True,pooling = 'attentive',conv = 'narrow'):\n",
      "#\n",
      "#        \"\"\"\n",
      "#            QA_RNN model for question answering\n",
      "#\n",
      "#            Args:\n",
      "#                self.dropout_keep_prob: dropout rate\n",
      "#                self.num_filters : number of filters\n",
      "#                self.para : parameter list\n",
      "#                self.extend_feature_dim : my extend feature dimension\n",
      "#                self.max_input_left : the length of question\n",
      "#                self.max_input_right : the length of answer\n",
      "#                self.pooling : pooling strategy :max pooling or attentive pooling\n",
      "#                \n",
      "#        \"\"\"\n",
      "#        self.dropout_keep_prob =  tf.placeholder(tf.float32,name = 'dropout_keep_prob')\n",
      "#        self.num_filters = num_filters\n",
      "#        self.embeddings = embeddings\n",
      "#        self.embedding_size = embedding_size\n",
      "#        self.batch_size = batch_size\n",
      "#        self.filter_sizes = filter_sizes\n",
      "#        self.l2_reg_lambda = l2_reg_lambda\n",
      "#        self.para = []\n",
      "#\n",
      "#        self.max_input_left = max_input_left\n",
      "#        self.max_input_right = max_input_right\n",
      "#        self.trainable = trainable\n",
      "#        self.vocab_size = vocab_size\n",
      "#        self.pooling = pooling\n",
      "#        self.total_num_filter = len(self.filter_sizes) * self.num_filters\n",
      "#\n",
      "#        self.conv = conv\n",
      "#        self.pooling = 'traditional'\n",
      "#        self.learning_rate = learning_rate\n",
      "#\n",
      "#        self.hidden_size = hidden_size\n",
      "#\n",
      "#        self.attention_size = 100\n",
      "    def __init__(self,opt):\n",
      "        for key,value in opt.items():\n",
      "            self.__setattr__(key,value)\n",
      "        self.attention_size = 100\n",
      "        self.pooling = 'mean'\n",
      "        self.total_num_filter = len(self.filter_sizes) * self.num_filters\n",
      "        self.para = []\n",
      "        self.dropout_keep_prob_holder =  tf.placeholder(tf.float32,name = 'dropout_keep_prob')\n",
      "    def create_placeholder(self):\n",
      "        print(('Create placeholders'))\n",
      "        # he length of the sentence is varied according to the batch,so the None,None\n",
      "        self.question = tf.placeholder(tf.int32,[None,None],name = 'input_question')\n",
      "        self.max_input_left = tf.shape(self.question)[1]\n",
      "   \n",
      "        self.batch_size = tf.shape(self.question)[0]\n",
      "        self.answer = tf.placeholder(tf.int32,[None,None],name = 'input_answer')\n",
      "        self.max_input_right = tf.shape(self.answer)[1]\n",
      "        self.answer_negative = tf.placeholder(tf.int32,[None,None],name = 'input_right')\n",
      "        # self.q_mask = tf.placeholder(tf.int32,[None,None],name = 'q_mask')\n",
      "        # self.a_mask = tf.placeholder(tf.int32,[None,None],name = 'a_mask')\n",
      "        # self.a_neg_mask = tf.placeholder(tf.int32,[None,None],name = 'a_neg_mask')\n",
      "\n",
      "    def add_embeddings(self):\n",
      "        print( 'add embeddings')\n",
      "        if self.embeddings is not None:\n",
      "            print( \"load embedding\")\n",
      "            W = tf.Variable(np.array(self.embeddings),name = \"W\" ,dtype=\"float32\",trainable = self.trainable)\n",
      "            \n",
      "        else:\n",
      "            print( \"random embedding\")\n",
      "            W = tf.Variable(tf.random_uniform([self.vocab_size, self.embedding_size], -1.0, 1.0),name=\"W\",trainable = self.trainable)\n",
      "        self.embedding_W = W\n",
      "       \n",
      "        # self.overlap_W = tf.Variable(a,name=\"W\",trainable = True)\n",
      "        self.para.append(self.embedding_W)\n",
      "\n",
      "        self.q_embedding =  tf.nn.embedding_lookup(self.embedding_W,self.question)\n",
      "\n",
      "\n",
      "        self.a_embedding = tf.nn.embedding_lookup(self.embedding_W,self.answer)\n",
      "        self.a_neg_embedding = tf.nn.embedding_lookup(self.embedding_W,self.answer_negative)\n",
      "        #real length\n",
      "        self.q_len,self.q_mask = blocks.length(self.question)\n",
      "        self.a_len,self.a_mask = blocks.length(self.answer)\n",
      "        self.a_neg_len,self.a_neg_mask = blocks.length(self.answer_negative)\n",
      "\n",
      "    def convolution(self):\n",
      "        print( 'convolution:wide_convolution')\n",
      "        self.kernels = []\n",
      "        for i,filter_size in enumerate(self.filter_sizes):\n",
      "            with tf.name_scope('conv-max-pool-%s' % filter_size):\n",
      "                filter_shape = [filter_size,self.embedding_size,1,self.num_filters]\n",
      "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev = 0.1), name=\"W\")\n",
      "                b = tf.Variable(tf.constant(0.0, shape=[self.num_filters]), name=\"b\")\n",
      "                self.kernels.append((W,b))\n",
      "                self.para.append(W)\n",
      "                self.para.append(b)\n",
      "       \n",
      "        embeddings = [self.q_embedding,self.a_embedding,self.a_neg_embedding]\n",
      "\n",
      "        self.q_cnn,self.a_cnn,self.a_neg_cnn = [self.wide_convolution(tf.expand_dims(embedding,-1)) for embedding in embeddings]\n",
      "\n",
      "        #convolution\n",
      "    def pooling_graph(self):\n",
      "        if self.pooling == 'mean':\n",
      "\n",
      "            self.q_pos_cnn = self.mean_pooling(self.q_cnn,self.q_mask)\n",
      "            self.q_neg_cnn = self.mean_pooling(self.q_cnn,self.q_mask)\n",
      "            self.a_pos_cnn = self.mean_pooling(self.a_cnn,self.a_mask)\n",
      "            self.a_neg_cnn = self.mean_pooling(self.a_neg_cnn,self.a_neg_mask)\n",
      "        elif self.pooling == 'attentive':\n",
      "            self.q_pos_cnn,self.a_pos_cnn = self.attentive_pooling(self.q_cnn,self.a_cnn,self.q_mask,self.a_mask)\n",
      "            self.q_neg_cnn,self.a_neg_cnn = self.attentive_pooling(self.q_cnn,self.a_neg_cnn,self.q_mask,self.a_neg_mask)\n",
      "        elif self.pooling == 'position':\n",
      "            self.q_pos_cnn,self.a_pos_cnn = self.position_attention(self.q_cnn,self.a_cnn,self.q_mask,self.a_mask)\n",
      "            self.q_neg_cnn,self.a_neg_cnn = self.position_attention(self.q_cnn,self.a_neg_cnn,self.q_mask,self.a_neg_mask)\n",
      "        elif self.pooling == 'traditional':\n",
      "            print( self.pooling)\n",
      "            print(self.q_cnn)\n",
      "            self.q_pos_cnn,self.a_pos_cnn = self.traditional_attention(self.q_cnn,self.a_cnn,self.q_mask,self.a_mask)\n",
      "            self.q_neg_cnn,self.a_neg_cnn = self.traditional_attention(self.q_cnn,self.a_neg_cnn,self.q_mask,self.a_neg_mask)\n",
      "\n",
      "    def para_initial(self):\n",
      "        # print((\"---------\"))\n",
      "        # self.W_qp = tf.Variable(tf.truncated_normal(shape = [self.hidden_size * 2,1],stddev = 0.01,name = 'W_qp'))\n",
      "        self.U = tf.Variable(tf.truncated_normal(shape = [self.total_num_filter,self.total_num_filter],stddev = 0.01,name = 'U'))\n",
      "        self.W_hm = tf.Variable(tf.truncated_normal(shape = [self.total_num_filter,self.total_num_filter],stddev = 0.01,name = 'W_hm'))\n",
      "        self.W_qm = tf.Variable(tf.truncated_normal(shape = [self.total_num_filter,self.total_num_filter],stddev = 0.01,name = 'W_qm'))\n",
      "        self.W_ms = tf.Variable(tf.truncated_normal(shape = [self.total_num_filter,1],stddev = 0.01,name = 'W_ms'))\n",
      "        self.M_qi = tf.Variable(tf.truncated_normal(shape = [self.total_num_filter,self.embedding_size],stddev = 0.01,name = 'M_qi'))\n",
      "\n",
      "\n",
      "\n",
      "    def mean_pooling(self,conv,mask):\n",
      "   \n",
      "        conv = tf.squeeze(conv,2)\n",
      "        print( tf.expand_dims(tf.cast(mask,tf.float32),-1))\n",
      "        # conv_mask = tf.multiply(conv,tf.expand_dims(tf.cast(mask,tf.float32),-1))\n",
      "        # self.see = conv_mask\n",
      "        # print( conv_mask)\n",
      "        return tf.reduce_mean(conv,axis = 1);\n",
      "    def attentive_pooling(self,input_left,input_right,q_mask,a_mask):\n",
      "\n",
      "        Q = tf.squeeze(input_left,axis = 2)\n",
      "        A = tf.squeeze(input_right,axis = 2)\n",
      "        print( Q)\n",
      "        print( A)\n",
      "        # Q = tf.reshape(input_left,[-1,self.max_input_left,len(self.filter_sizes) * self.num_filters],name = 'Q')\n",
      "        # A = tf.reshape(input_right,[-1,self.max_input_right,len(self.filter_sizes) * self.num_filters],name = 'A')\n",
      "        # G = tf.tanh(tf.matmul(tf.matmul(Q,self.U),\\\n",
      "        # A,transpose_b = True),name = 'G')\n",
      "        \n",
      "        first = tf.matmul(tf.reshape(Q,[-1,len(self.filter_sizes) * self.num_filters]),self.U)\n",
      "        second_step = tf.reshape(first,[-1,self.max_input_left,len(self.filter_sizes) * self.num_filters])\n",
      "        result = tf.matmul(second_step,tf.transpose(A,perm = [0,2,1]))\n",
      "        print( second_step)\n",
      "        print( tf.transpose(A,perm = [0,2,1]))\n",
      "        # print( 'result',result)\n",
      "        G = tf.tanh(result)\n",
      "        \n",
      "        # G = result\n",
      "        # column-wise pooling ,row-wise pooling\n",
      "        row_pooling = tf.reduce_max(G,1,True,name = 'row_pooling')\n",
      "        col_pooling = tf.reduce_max(G,2,True,name = 'col_pooling')\n",
      "    \n",
      "        self.attention_q = tf.nn.softmax(col_pooling,1,name = 'attention_q')\n",
      "        self.attention_q_mask = tf.multiply(self.attention_q,tf.expand_dims(tf.cast(q_mask,tf.float32),-1))\n",
      "        self.attention_a = tf.nn.softmax(row_pooling,name = 'attention_a')\n",
      "        self.attention_a_mask = tf.multiply(self.attention_a,tf.expand_dims(tf.cast(a_mask,tf.float32),1))\n",
      "        \n",
      "        self.see = G\n",
      "\n",
      "        R_q = tf.reshape(tf.matmul(Q,self.attention_q_mask,transpose_a = 1),[-1,self.num_filters * len(self.filter_sizes)],name = 'R_q')\n",
      "        R_a = tf.reshape(tf.matmul(self.attention_a_mask,A),[-1,self.num_filters * len(self.filter_sizes)],name = 'R_a')\n",
      "\n",
      "        return R_q,R_a\n",
      "\n",
      "    def traditional_attention(self,input_left,input_right,q_mask,a_mask):\n",
      "        input_left = tf.squeeze(input_left,axis = 2)\n",
      "        input_right = tf.squeeze(input_right,axis = 2) \n",
      "\n",
      "        input_left_mask = tf.multiply(input_left, tf.expand_dims(tf.cast(q_mask,tf.float32),2))\n",
      "        Q = tf.reduce_mean(input_left_mask,1)\n",
      "        a_shape = tf.shape(input_right)\n",
      "        A = tf.reshape(input_right,[-1,self.total_num_filter])\n",
      "        m_t = tf.nn.tanh(tf.reshape(tf.matmul(A,self.W_hm),[-1,a_shape[1],self.total_num_filter]) + tf.expand_dims(tf.matmul(Q,self.W_qm),1))\n",
      "        f_attention = tf.exp(tf.reshape(tf.matmul(tf.reshape(m_t,[-1,self.total_num_filter]),self.W_ms),[-1,a_shape[1],1]))\n",
      "        self.f_attention_mask = tf.multiply(f_attention,tf.expand_dims(tf.cast(a_mask,tf.float32),2))\n",
      "        self.f_attention_norm = tf.divide(self.f_attention_mask,tf.reduce_sum(self.f_attention_mask,1,keep_dims = True))\n",
      "        self.see = self.f_attention_norm\n",
      "        a_attention = tf.reduce_sum(tf.multiply(input_right,self.f_attention_norm),1)\n",
      "        return Q,a_attention\n",
      "    def position_attention(self,input_left,input_right,q_mask,a_mask):\n",
      "        input_left = tf.squeeze(input_left,axis = 2)\n",
      "        input_right = tf.squeeze(input_right,axis = 2)\n",
      "        # Q = tf.reshape(input_left,[-1,self.max_input_left,self.hidden_size*2],name = 'Q')\n",
      "        # A = tf.reshape(input_right,[-1,self.max_input_right,self.hidden_size*2],name = 'A')\n",
      "\n",
      "        Q = tf.reduce_mean(tf.multiply(input_left,tf.expand_dims(tf.cast(self.q_mask,tf.float32),2)),1)\n",
      "\n",
      "        QU = tf.matmul(Q,self.U)\n",
      "        QUA = tf.multiply(tf.expand_dims(QU,1),input_right)\n",
      "        self.attention_a = tf.cast(tf.argmax(QUA,2)\n",
      "            ,tf.float32)\n",
      "        # q_shape = tf.shape(input_left)\n",
      "        # Q_1 = tf.reshape(input_left,[-1,self.total_num_filter])\n",
      "        # QU = tf.matmul(Q_1,self.U)\n",
      "        # QU_1 = tf.reshape(QU,[-1,q_shape[1],self.total_num_filter])\n",
      "        # A_1 = tf.transpose(input_right,[0,2,1])\n",
      "        # QUA = tf.matmul(QU_1,A_1)\n",
      "        # QUA = tf.nn.l2_normalize(QUA,1)\n",
      "\n",
      "        # G = tf.tanh(QUA)\n",
      "        # Q = tf.reduce_mean(tf.multiply(input_left,tf.expand_dims(tf.cast(self.q_mask,tf.float32),2)),1)\n",
      "        # # self.Q_mask = tf.multiply(input_left,tf.expand_dims(tf.cast(self.q_mask,tf.float32),2))\n",
      "        # row_pooling = tf.reduce_max(G,1,name=\"row_pooling\")\n",
      "        # col_pooling = tf.reduce_max(G,2,name=\"col_pooling\")\n",
      "        # self.attention_a = tf.nn.softmax(row_pooling,1,name = \"attention_a\")\n",
      "        self.attention_a_mask = tf.multiply(self.attention_a,tf.cast(a_mask,tf.float32))\n",
      "        self.see = self.attention_a\n",
      "        self.attention_a_norm = tf.divide(self.attention_a_mask,tf.reduce_sum(self.attention_a_mask,1,keep_dims =True))\n",
      "        self.r_a = tf.reshape(tf.matmul(tf.transpose(input_right,[0,2,1]) ,tf.expand_dims(self.attention_a_norm,2)),[-1,self.total_num_filter])\n",
      "        return Q ,self.r_a\n",
      "    def create_loss(self):\n",
      "        \n",
      "        with tf.name_scope('score'):\n",
      "            self.score12 = self.getCosine(self.q_pos_cnn,self.a_pos_cnn)\n",
      "            self.score13 = self.getCosine(self.q_neg_cnn,self.a_neg_cnn)\n",
      "        l2_loss = tf.constant(0.0)\n",
      "        for p in self.para:\n",
      "            l2_loss += tf.nn.l2_loss(p)\n",
      "        with tf.name_scope(\"loss\"):\n",
      "            self.losses = tf.maximum(0.0, tf.subtract(0.05, tf.subtract(self.score12, self.score13)))\n",
      "            self.loss = tf.reduce_sum(self.losses) + self.l2_reg_lambda * l2_loss\n",
      "        tf.summary.scalar('loss', self.loss)\n",
      "        # Accuracy\n",
      "        with tf.name_scope(\"accuracy\"):\n",
      "            self.correct = tf.equal(0.0, self.losses)\n",
      "            self.accuracy = tf.reduce_mean(tf.cast(self.correct, \"float\"), name=\"accuracy\")\n",
      "        tf.summary.scalar('accuracy', self.accuracy)\n",
      "    def create_op(self):\n",
      "        self.global_step = tf.Variable(0, name = \"global_step\", trainable = False)\n",
      "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
      "        self.grads_and_vars = self.optimizer.compute_gradients(self.loss)\n",
      "        self.train_op = self.optimizer.apply_gradients(self.grads_and_vars, global_step = self.global_step)\n",
      "\n",
      "\n",
      "    def max_pooling(self,conv,input_length):\n",
      "        pooled = tf.nn.max_pool(\n",
      "                    conv,\n",
      "                    ksize = [1, input_length, 1, 1],\n",
      "                    strides = [1, 1, 1, 1],\n",
      "                    padding = 'VALID',\n",
      "                    name=\"pool\")\n",
      "        return pooled\n",
      "    def getCosine(self,q,a):\n",
      "        pooled_flat_1 = tf.nn.dropout(q, self.dropout_keep_prob_holder)\n",
      "        pooled_flat_2 = tf.nn.dropout(a, self.dropout_keep_prob_holder)\n",
      "        \n",
      "        pooled_len_1 = tf.sqrt(tf.reduce_sum(tf.multiply(pooled_flat_1, pooled_flat_1), 1)) \n",
      "        pooled_len_2 = tf.sqrt(tf.reduce_sum(tf.multiply(pooled_flat_2, pooled_flat_2), 1))\n",
      "        pooled_mul_12 = tf.reduce_sum(tf.multiply(pooled_flat_1, pooled_flat_2), 1) \n",
      "        score = tf.div(pooled_mul_12, tf.multiply(pooled_len_1, pooled_len_2), name=\"scores\") \n",
      "        return score\n",
      "    def wide_convolution(self,embedding):\n",
      "        cnn_outputs = []\n",
      "        for i,filter_size in enumerate(self.filter_sizes):\n",
      "            conv = tf.nn.conv2d(\n",
      "                    embedding,\n",
      "                    self.kernels[i][0],\n",
      "                    strides=[1, 1, self.embedding_size, 1],\n",
      "                    padding='SAME',\n",
      "                    name=\"conv-1\"\n",
      "            )\n",
      "            h = tf.nn.relu(tf.nn.bias_add(conv, self.kernels[i][1]), name=\"relu-1\")\n",
      "            cnn_outputs.append(h)\n",
      "        cnn_reshaped = tf.concat(cnn_outputs,3)\n",
      "        return cnn_reshaped\n",
      "    \n",
      "    def variable_summaries(self,var):\n",
      "        with tf.name_scope('summaries'):\n",
      "            mean = tf.reduce_mean(var)\n",
      "            tf.summary.scalar('mean', mean)\n",
      "            with tf.name_scope('stddev'):\n",
      "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
      "            tf.summary.scalar('stddev', stddev)\n",
      "            tf.summary.scalar('max', tf.reduce_max(var))\n",
      "            tf.summary.scalar('min', tf.reduce_min(var))\n",
      "            tf.summary.histogram('histogram', var)\n",
      "\n",
      "    def build_graph(self):\n",
      "        self.create_placeholder()\n",
      "        self.add_embeddings()\n",
      "        self.para_initial()\n",
      "        self.convolution()\n",
      "        self.pooling_graph()\n",
      "        self.create_loss()\n",
      "        self.create_op()\n",
      "        self.merged = tf.summary.merge_all()\n",
      "\n",
      "    def train(self,sess,data):\n",
      "        feed_dict = {\n",
      "                self.question:data[0],\n",
      "                self.answer:data[1],\n",
      "                self.answer_negative:data[2],\n",
      "                # self.q_mask:data[3],\n",
      "                # self.a_mask:data[4],\n",
      "                # self.a_neg_mask:data[5],\n",
      "                self.dropout_keep_prob_holder:self.dropout_keep_prob\n",
      "            }\n",
      "\n",
      "        _, summary, step, loss, accuracy,score12, score13, see = sess.run(\n",
      "                    [self.train_op, self.merged,self.global_step,self.loss, self.accuracy,self.score12,self.score13, self.see],\n",
      "                    feed_dict)\n",
      "        return _, summary, step, loss, accuracy,score12, score13, see\n",
      "    def predict(self,sess,data):\n",
      "        feed_dict = {\n",
      "                self.question:data[0],\n",
      "                self.answer:data[1],\n",
      "                # self.q_mask:data[2],\n",
      "                # self.a_mask:data[3],\n",
      "                self.dropout_keep_prob_holder:1.0\n",
      "            }            \n",
      "        score = sess.run( self.score12, feed_dict)       \n",
      "        return score\n",
      "\n",
      "    \n",
      "if __name__ == '__main__':\n",
      "    \n",
      "    cnn = QA_CNN_extend(\n",
      "        max_input_left = 33,\n",
      "        max_input_right = 40,\n",
      "        batch_size = 3,\n",
      "        vocab_size = 5000,\n",
      "        embedding_size = 100,\n",
      "        filter_sizes = [3,4,5],\n",
      "        num_filters = 64, \n",
      "        hidden_size = 100,\n",
      "        dropout_keep_prob = 1.0,\n",
      "        embeddings = None,\n",
      "        l2_reg_lambda = 0.0,\n",
      "        trainable = True,\n",
      "\n",
      "        pooling = 'max',\n",
      "        conv = 'wide')\n",
      "    cnn.build_graph()\n",
      "    input_x_1 = np.reshape(np.arange(3 * 33),[3,33])\n",
      "    input_x_2 = np.reshape(np.arange(3 * 40),[3,40])\n",
      "    input_x_3 = np.reshape(np.arange(3 * 40),[3,40])\n",
      "    q_mask = np.ones((3,33))\n",
      "    a_mask = np.ones((3,40))\n",
      "    a_neg_mask = np.ones((3,40))\n",
      "  \n",
      "\n",
      "    with tf.Session() as sess:\n",
      "        sess.run(tf.global_variables_initializer())\n",
      "        feed_dict = {\n",
      "            cnn.question:input_x_1,\n",
      "            cnn.answer:input_x_2,\n",
      "            # cnn.answer_negative:input_x_3,\n",
      "            cnn.q_mask:q_mask,\n",
      "            cnn.a_mask:a_mask,\n",
      "            cnn.dropout_keep_prob_holder:cnn.dropout_keep\n",
      "            # cnn.a_neg_mask:a_neg_mask\n",
      "            # cnn.q_pos_overlap:q_pos_embedding,\n",
      "            # cnn.q_neg_overlap:q_neg_embedding,\n",
      "            # cnn.a_pos_overlap:a_pos_embedding,\n",
      "            # cnn.a_neg_overlap:a_neg_embedding,\n",
      "            # cnn.q_position:q_position,\n",
      "            # cnn.a_pos_position:a_pos_position,\n",
      "            # cnn.a_neg_position:a_neg_position\n",
      "        }\n",
      "        question,answer,score = sess.run([cnn.question,cnn.answer,cnn.score12],feed_dict)\n",
      "        print( question.shape,answer.shape)\n",
      "        print( score)\n",
      "\n",
      "\n",
      "\n",
      "from my.general import flatten, reconstruct, add_wd, exp_mask\n",
      "\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "_BIAS_VARIABLE_NAME = \"bias\"\n",
      "_WEIGHTS_VARIABLE_NAME = \"kernel\"\n",
      "\n",
      "\n",
      "\n",
      "def linear(args, output_size, bias, bias_start=0.0, scope=None, squeeze=False, wd=0.0, input_keep_prob=1.0,\n",
      "           is_train=None):#, name_w='', name_b=''\n",
      "    # if args is None or (nest.is_sequence(args) and not args):\n",
      "    #     raise ValueError(\"`args` must be specified\")\n",
      "    # if not nest.is_sequence(args):\n",
      "    #     args = [args]\n",
      "\n",
      "    flat_args = [flatten(arg, 1) for arg in args]#[210,20]\n",
      "\n",
      "    # if input_keep_prob < 1.0:\n",
      "    #     assert is_train is not None\n",
      "    flat_args = [tf.nn.dropout(arg, input_keep_prob) for arg in flat_args]\n",
      "    \n",
      "    total_arg_size = 0#[60]\n",
      "    shapes = [a.get_shape() for a in flat_args]\n",
      "    for shape in shapes:\n",
      "        if shape.ndims != 2:\n",
      "            raise ValueError(\"linear is expecting 2D arguments: %s\" % shapes)\n",
      "        if shape[1].value is None:\n",
      "            raise ValueError(\"linear expects shape[1] to be provided for shape %s, \"\n",
      "                       \"but saw %s\" % (shape, shape[1]))\n",
      "        else:\n",
      "            total_arg_size += shape[1].value\n",
      "    # print(total_arg_size)\n",
      "    # exit()\n",
      "    dtype = [a.dtype for a in flat_args][0]        \n",
      "\n",
      "    # scope = tf.get_variable_scope()\n",
      "    with tf.variable_scope(scope) as outer_scope:\n",
      "        weights = tf.get_variable(_WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size], dtype=dtype)\n",
      "        if len(flat_args) == 1:\n",
      "            res = tf.matmul(flat_args[0], weights)\n",
      "        else: \n",
      "            res = tf.matmul(tf.concat(flat_args, 1), weights)\n",
      "        if not bias:\n",
      "            flat_out = res\n",
      "        else:\n",
      "            with tf.variable_scope(outer_scope) as inner_scope:\n",
      "                inner_scope.set_partitioner(None)\n",
      "                biases = tf.get_variable(\n",
      "                    _BIAS_VARIABLE_NAME, [output_size],\n",
      "                    dtype=dtype,\n",
      "                    initializer=tf.constant_initializer(bias_start, dtype=dtype))\n",
      "            flat_out = tf.nn.bias_add(res, biases)    \n",
      "\n",
      "    out = reconstruct(flat_out, args[0], 1)\n",
      "\n",
      "    if squeeze:\n",
      "        out = tf.squeeze(out, [len(args[0].get_shape().as_list())-1])\n",
      "    if wd:\n",
      "        add_wd(wd)\n",
      "\n",
      "    return out\n",
      "\n",
      "def softmax(logits, mask=None, scope=None):\n",
      "    with tf.name_scope(scope or \"Softmax\"):\n",
      "        if mask is not None:\n",
      "            logits = exp_mask(logits, mask)\n",
      "        flat_logits = flatten(logits, 1)\n",
      "        flat_out = tf.nn.softmax(flat_logits)\n",
      "        out = reconstruct(flat_out, logits, 1)\n",
      "\n",
      "        return out\n",
      "\n",
      "\n",
      "def softsel(target, logits, mask=None, scope=None):\n",
      "    \"\"\"\n",
      "\n",
      "    :param target: [ ..., J, d] dtype=float\n",
      "    :param logits: [ ..., J], dtype=float\n",
      "    :param mask: [ ..., J], dtype=bool\n",
      "    :param scope:\n",
      "    :return: [..., d], dtype=float\n",
      "    \"\"\"\n",
      "    with tf.name_scope(scope or \"Softsel\"):\n",
      "        a = softmax(logits, mask = mask)\n",
      "        target_rank = len(target.get_shape().as_list())\n",
      "        out = tf.reduce_sum(tf.expand_dims(a, -1) * target, target_rank - 2)\n",
      "        return out\n",
      "\n",
      "def highway_layer(arg, bias, bias_start=0.0, scope=None, wd=0.0, input_keep_prob=1.0):\n",
      "    with tf.variable_scope(scope or \"highway_layer\"):\n",
      "        d = arg.get_shape()[-1]\n",
      "        trans = linear([arg], d, bias, bias_start=bias_start, scope='trans', wd=wd, input_keep_prob=input_keep_prob)\n",
      "        trans = tf.nn.relu(trans)\n",
      "        gate = linear([arg], d, bias, bias_start=bias_start, scope='gate', wd=wd, input_keep_prob=input_keep_prob)\n",
      "        gate = tf.nn.sigmoid(gate)\n",
      "        out = gate * trans + (1 - gate) * arg\n",
      "        return out\n",
      "\n",
      "\n",
      "def highway_network(arg, num_layers, bias, bias_start=0.0, scope=None, wd=0.0, input_keep_prob=1.0):\n",
      "    with tf.variable_scope(scope or \"highway_network\"):\n",
      "        prev = arg\n",
      "        cur = None\n",
      "        for layer_idx in range(num_layers):\n",
      "            cur = highway_layer(prev, bias, bias_start=bias_start, scope=\"layer_{}\".format(layer_idx), wd=wd,\n",
      "                                input_keep_prob=input_keep_prob)\n",
      "            prev = cur\n",
      "        return cur\n",
      "\n",
      "def conv1d(in_, filter_size, height, padding, keep_prob=1.0, scope=None):\n",
      "    with tf.variable_scope(scope or \"conv1d\"):\n",
      "        num_channels = in_.get_shape()[-1]\n",
      "        filter_ = tf.get_variable(\"filter\", shape=[1, height, num_channels, filter_size], dtype='float')\n",
      "        bias = tf.get_variable(\"bias\", shape=[filter_size], dtype='float')\n",
      "        strides = [1, 1, 1, 1]\n",
      "        in_ = tf.nn.dropout(in_, keep_prob)\n",
      "        xxc = tf.nn.conv2d(in_, filter_, strides, padding) + bias  # [N*M, JX, W/filter_stride, d]\n",
      "        out = tf.reduce_max(tf.nn.relu(xxc), 2)  # [-1, JX, d]\n",
      "        return out\n",
      "\n",
      "\n",
      "def multi_conv1d(in_, filter_sizes, heights, padding, keep_prob=1.0, scope=None):\n",
      "    with tf.variable_scope(scope or \"multi_conv1d\"):\n",
      "        assert len(filter_sizes) == len(heights)\n",
      "        outs = []\n",
      "        for filter_size, height in zip(filter_sizes, heights):\n",
      "            if filter_size == 0:\n",
      "                continue\n",
      "            out = conv1d(in_, filter_size, height, padding, keep_prob=keep_prob, scope=\"conv1d_{}\".format(height))\n",
      "            outs.append(out)\n",
      "        concat_out = tf.concat(outs, axis=2)\n",
      "        return concat_out\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    a = tf.Variable(np.random.random(size=(2,2,4)))\n",
      "    b = tf.Variable(np.random.random(size=(2,3,4)))\n",
      "    c = tf.tile(tf.expand_dims(a, 2), [1, 1, 3, 1])\n",
      "    test = flatten(c,1)\n",
      "    out = reconstruct(test, c, 1)\n",
      "    d = tf.tile(tf.expand_dims(b, 1), [1, 2, 1, 1])\n",
      "    e = linear([c,d,c*d],1,bias = False,scope = \"test\",)\n",
      "    # f = softsel(d, e)\n",
      "    with tf.Session() as sess:\n",
      "        tf.global_variables_initializer().run()\n",
      "        print(sess.run(test))\n",
      "        print(sess.run(tf.shape(out)))\n",
      "        exit()\n",
      "        print(sess.run(tf.shape(a)))\n",
      "        print(sess.run(a))\n",
      "        print(sess.run(tf.shape(b)))\n",
      "        print(sess.run(b))\n",
      "        print(sess.run(tf.shape(c)))\n",
      "        print(sess.run(c))  \n",
      "        print(sess.run(tf.shape(d)))\n",
      "        print(sess.run(d))\n",
      "        print(sess.run(tf.shape(e)))\n",
      "        print(sess.run(e))\n",
      "\n",
      "from .QA_CNN_pairwise import QA_CNN_extend as CNN\n",
      "from .QA_RNN_pairwise import QA_RNN_extend as RNN\n",
      "from .QA_CNN_quantum_pairwise import QA_CNN_extend as QCNN\n",
      "def setup(opt):\n",
      "\tif opt[\"model_name\"]==\"cnn\":\n",
      "\t\tmodel=CNN(opt)\n",
      "\telif opt[\"model_name\"]==\"rnn\":\n",
      "\t\tmodel=RNN(opt)\n",
      "\telif opt['model_name']=='qcnn':\n",
      "\t\tmodel=QCNN(opt)\n",
      "\telse:\n",
      "\t\tprint(\"no model\")\n",
      "\t\texit(0)\n",
      "\treturn model\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "from tensorflow import flags\n",
      "import tensorflow as tf\n",
      "from config import Singleton\n",
      "import data_helper\n",
      "\n",
      "import datetime\n",
      "import os\n",
      "import models\n",
      "import numpy as np\n",
      "import evaluation\n",
      "\n",
      "from data_helper import log_time_delta,getLogger\n",
      "\n",
      "logger=getLogger()\n",
      "    \n",
      "\n",
      "\n",
      "args = Singleton().get_rnn_flag()\n",
      "#args = Singleton().get_8008_flag()\n",
      "\n",
      "args._parse_flags()\n",
      "opts=dict()\n",
      "logger.info(\"\\nParameters:\")\n",
      "for attr, value in sorted(args.__flags.items()):\n",
      "    logger.info((\"{}={}\".format(attr.upper(), value)))\n",
      "    opts[attr]=value\n",
      "\n",
      "\n",
      "train,test,dev = data_helper.load(args.data,filter = args.clean)\n",
      "\n",
      "q_max_sent_length = max(map(lambda x:len(x),train['question'].str.split()))\n",
      "a_max_sent_length = max(map(lambda x:len(x),train['answer'].str.split()))\n",
      "\n",
      "alphabet = data_helper.get_alphabet([train,test,dev],dataset=args.data )\n",
      "logger.info('the number of words :%d '%len(alphabet))\n",
      "\n",
      "if args.data==\"quora\" or  args.data==\"8008\" :\n",
      "    print(\"cn embedding\")\n",
      "    embedding = data_helper.get_embedding(alphabet,dim=200,language=\"cn\",dataset=args.data )\n",
      "    train_data_loader = data_helper.getBatch48008\n",
      "else:\n",
      "    embedding = data_helper.get_embedding(alphabet,dim=300,dataset=args.data )\n",
      "    train_data_loader = data_helper.get_mini_batch\n",
      "opts[\"embeddings\"] =embedding\n",
      "opts[\"vocab_size\"]=len(alphabet)\n",
      "opts[\"max_input_right\"]=a_max_sent_length\n",
      "opts[\"max_input_left\"]=q_max_sent_length\n",
      "opts[\"filter_sizes\"]=list(map(int, args.filter_sizes.split(\",\")))\n",
      "\n",
      "print(\"innitilize over\")\n",
      "\n",
      "\n",
      "   \n",
      " \n",
      "#with tf.Graph().as_default(), tf.device(\"/gpu:\" + str(args.gpu)):\n",
      "with tf.Graph().as_default():    \n",
      "    # with tf.device(\"/cpu:0\"):\n",
      "    session_conf = tf.ConfigProto()\n",
      "    session_conf.allow_soft_placement = args.allow_soft_placement\n",
      "    session_conf.log_device_placement = args.log_device_placement\n",
      "    session_conf.gpu_options.allow_growth = True\n",
      "    sess = tf.Session(config=session_conf)\n",
      "    model=models.setup(opts)\n",
      "    model.build_graph()    \n",
      "    saver = tf.train.Saver()\n",
      "    sess.run(tf.global_variables_initializer())  # fun first than print or save\n",
      "    \n",
      "    \n",
      "    ckpt = tf.train.get_checkpoint_state(\"checkpoint\")    \n",
      "    if ckpt and ckpt.model_checkpoint_path:    \n",
      "        # Restores from checkpoint    \n",
      "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
      "    print(sess.run(model.position_embedding)[0])\n",
      "    if os.path.exists(\"model\") :                        \n",
      "        import shutil\n",
      "        shutil.rmtree(\"model\")\n",
      "    builder = tf.saved_model.builder.SavedModelBuilder(\"./model\")\n",
      "    builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING])\n",
      "    builder.save(True)\n",
      "    variable_averages = tf.train.ExponentialMovingAverage(  model)    \n",
      "    variables_to_restore = variable_averages.variables_to_restore()    \n",
      "    saver = tf.train.Saver(variables_to_restore)  \n",
      "    for name in variables_to_restore:    \n",
      "        print(name) \n",
      "   \n",
      "    @log_time_delta\n",
      "    def predict(model,sess,batch,test):\n",
      "        scores = []\n",
      "        for data in batch:            \n",
      "            score = model.predict(sess,data)\n",
      "            scores.extend(score)  \n",
      "        return np.array(scores[:len(test)])\n",
      "    \n",
      "    \n",
      "    text = \"怎么 提取 公积金 ？\"\n",
      "  \n",
      "    splited_text=data_helper.encode_to_split(text,alphabet)\n",
      "\n",
      "    mb_q,mb_q_mask = data_helper.prepare_data([splited_text])\n",
      "    mb_a,mb_a_mask = data_helper.prepare_data([splited_text])\n",
      "    \n",
      "    data = (mb_q,mb_a,mb_q_mask,mb_a_mask)\n",
      "    score = model.predict(sess,data)\n",
      "    print(score)\n",
      "    feed_dict = {\n",
      "                model.question:data[0],\n",
      "                model.answer:data[1],\n",
      "                model.q_mask:data[2],\n",
      "                model.a_mask:data[3],\n",
      "                model.dropout_keep_prob_holder:1.0\n",
      "            }   \n",
      "    sess.run(model.position_embedding,feed_dict=feed_dict)[0]\n",
      "\n",
      "    \n",
      "   \n",
      "#-*- coding:utf-8 -*-\n",
      "\n",
      "import os\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "import string\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "\n",
      "from tqdm import tqdm\n",
      "import random\n",
      "from functools import wraps\n",
      "import time\n",
      "import pickle\n",
      "def log_time_delta(func):\n",
      "    @wraps(func)\n",
      "    def _deco(*args, **kwargs):\n",
      "        start = time.time()\n",
      "        ret = func(*args, **kwargs)\n",
      "        end = time.time()\n",
      "        delta = end - start\n",
      "        print( \"%s runed %.2f seconds\"% (func.__name__,delta))\n",
      "        return ret\n",
      "    return _deco\n",
      "\n",
      "import tqdm\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "\n",
      "OVERLAP = 237\n",
      "class Alphabet(dict):\n",
      "    def __init__(self, start_feature_id = 1):\n",
      "        self.fid = start_feature_id\n",
      "\n",
      "    def add(self, item):\n",
      "        idx = self.get(item, None)\n",
      "        if idx is None:\n",
      "            idx = self.fid\n",
      "            self[item] = idx\n",
      "      # self[idx] = item\n",
      "            self.fid += 1\n",
      "        return idx\n",
      "\n",
      "    def dump(self, fname):\n",
      "        with open(fname, \"w\") as out:\n",
      "            for k in sorted(self.keys()):\n",
      "                out.write(\"{}\\t{}\\n\".format(k, self[k]))\n",
      "def cut(sentence):\n",
      "    \n",
      "    tokens = sentence.lower().split()\n",
      "    # tokens = [w for w in tokens if w not in stopwords.words('english')]\n",
      "    return tokens\n",
      "@log_time_delta\n",
      "def load(dataset, filter = False):\n",
      "    data_dir = \"data/\" + dataset\n",
      "    datas = []\n",
      "    for data_name in ['train.txt','test.txt','dev.txt']:\n",
      "        data_file = os.path.join(data_dir,data_name)\n",
      "        data = pd.read_csv(data_file,header = None,sep=\"\\t\",names=[\"question\",\"answer\",\"flag\"]).fillna('0')\n",
      "#        data = pd.read_csv(data_file,header = None,sep=\"\\t\",names=[\"question\",\"answer\",\"flag\"],quoting =3).fillna('0')\n",
      "        if filter == True:\n",
      "            datas.append(removeUnanswerdQuestion(data))\n",
      "        else:\n",
      "            datas.append(data)\n",
      "    # sub_file = os.path.join(data_dir,'submit.txt')\n",
      "    # submit = pd.read_csv(sub_file,header = None,sep = \"\\t\",names = ['question','answer'],quoting = 3)\n",
      "    # datas.append(submit)\n",
      "    return tuple(datas)\n",
      "@log_time_delta\n",
      "def removeUnanswerdQuestion(df):\n",
      "    counter= df.groupby(\"question\").apply(lambda group: sum(group[\"flag\"]))\n",
      "    questions_have_correct=counter[counter>0].index\n",
      "    counter= df.groupby(\"question\").apply(lambda group: sum(group[\"flag\"]==0))\n",
      "    questions_have_uncorrect=counter[counter>0].index\n",
      "    counter=df.groupby(\"question\").apply(lambda group: len(group[\"flag\"]))\n",
      "    questions_multi=counter[counter>1].index\n",
      "\n",
      "    return df[df[\"question\"].isin(questions_have_correct) &  df[\"question\"].isin(questions_have_correct) & df[\"question\"].isin(questions_have_uncorrect)].reset_index()\n",
      "@log_time_delta\n",
      "def get_alphabet(corpuses=None,dataset=\"\"):\n",
      "    pkl_name=\"temp/\"+dataset+\".alphabet.pkl\"\n",
      "    if  os.path.exists(pkl_name):\n",
      "        return pickle.load(open(pkl_name,\"rb\"))\n",
      "    alphabet = Alphabet(start_feature_id = 0)\n",
      "    alphabet.add('[UNK]')  \n",
      "    alphabet.add('END') \n",
      "    count = 0\n",
      "    for corpus in corpuses:\n",
      "        for texts in [corpus[\"question\"].unique(),corpus[\"answer\"]]:\n",
      "\n",
      "            for sentence in texts:                   \n",
      "                tokens = cut(sentence)\n",
      "                for token in set(tokens):\n",
      "                    alphabet.add(token)\n",
      "        print(\"alphabet size %d\" % len(alphabet.keys()) )\n",
      "    if not os.path.exists(\"temp\"):\n",
      "        os.mkdir(\"temp\")\n",
      "    pickle.dump( alphabet,open(pkl_name,\"wb\"))\n",
      "    return alphabet\n",
      "@log_time_delta\n",
      "def getSubVectorsFromDict(vectors,vocab,dim = 300):\n",
      "    embedding = np.zeros((len(vocab),dim))\n",
      "    count = 1\n",
      "    for word in vocab:\n",
      "        if word in vectors:\n",
      "            count += 1\n",
      "            embedding[vocab[word]]= vectors[word]\n",
      "        else:\n",
      "            embedding[vocab[word]]= np.random.uniform(-0.5,+0.5,dim)#vectors['[UNKNOW]'] #.tolist()\n",
      "    print( 'word in embedding',count)\n",
      "    return embedding\n",
      "def encode_to_split(sentence,alphabet):\n",
      "    indices = []    \n",
      "    tokens = cut(sentence)\n",
      "    seq = [alphabet[w] if w in alphabet else alphabet['[UNK]'] for w in tokens]\n",
      "    return seq\n",
      "@log_time_delta\n",
      "def load_text_vec(alphabet,filename=\"\",embedding_size = 100):\n",
      "    vectors = {}\n",
      "    with open(filename,encoding='utf-8') as f:\n",
      "        i = 0\n",
      "        for line in f:\n",
      "            i += 1\n",
      "            if i % 100000 == 0:\n",
      "                print( 'epch %d' % i)\n",
      "            items = line.strip().split(' ')\n",
      "            if len(items) == 2:\n",
      "                vocab_size, embedding_size= items[0],items[1]\n",
      "                print( ( vocab_size, embedding_size))\n",
      "            else:\n",
      "                word = items[0]\n",
      "                if word in alphabet:\n",
      "                    vectors[word] = items[1:]\n",
      "    print( 'embedding_size',embedding_size)\n",
      "    print( 'done')\n",
      "    print( 'words found in wor2vec embedding ',len(vectors.keys()))\n",
      "    return vectors\n",
      "@log_time_delta\n",
      "def get_embedding(alphabet,dim = 300,language =\"en\",dataset=\"\"):\n",
      "    pkl_name=\"temp/\"+dataset+\".subembedding.pkl\"\n",
      "    if  os.path.exists(pkl_name):\n",
      "        return pickle.load(open(pkl_name,\"rb\"))\n",
      "    if language==\"en\":\n",
      "        fname = 'embedding/glove.6B/glove.6B.300d.txt'\n",
      "    else:\n",
      "        fname= \"embedding/embedding.200.header_txt\"\n",
      "    embeddings = load_text_vec(alphabet,fname,embedding_size = dim)\n",
      "    sub_embeddings = getSubVectorsFromDict(embeddings,alphabet,dim)\n",
      "    pickle.dump( sub_embeddings,open(pkl_name,\"wb\"))\n",
      "    return sub_embeddings\n",
      "\n",
      "@log_time_delta\n",
      "def get_mini_batch_test(df,alphabet,batch_size):\n",
      "    q = []\n",
      "    a = []\n",
      "    pos_overlap = []\n",
      "    for index,row in df.iterrows():\n",
      "        question = encode_to_split(row[\"question\"],alphabet)\n",
      "        answer = encode_to_split(row[\"answer\"],alphabet)\n",
      "        overlap_pos = overlap_index(row['question'],row['answer'])\n",
      "        q.append(question)\n",
      "        a.append(answer)\n",
      "        pos_overlap.append(overlap_pos)\n",
      "\n",
      "    m = 0\n",
      "    n = len(q)\n",
      "    idx_list = np.arange(m,n,batch_size)\n",
      "    mini_batches = []\n",
      "    for idx in idx_list:\n",
      "        mini_batches.append(np.arange(idx,min(idx + batch_size,n)))\n",
      "    for mini_batch in mini_batches:\n",
      "        mb_q = [ q[t] for t in mini_batch]\n",
      "        mb_a = [ a[t] for t in mini_batch]\n",
      "        mb_pos_overlap = [pos_overlap[t] for t in mini_batch]\n",
      "        mb_q,mb_q_mask = prepare_data(mb_q)\n",
      "        mb_a,mb_pos_overlaps = prepare_data(mb_a,mb_pos_overlap)\n",
      "\n",
      "\n",
      "        yield(mb_q,mb_a)\n",
      "\n",
      "# calculate the overlap_index\n",
      "def overlap_index(question,answer,stopwords = []):\n",
      "    ans_token = cut(answer)\n",
      "    qset = set(cut(question))\n",
      "    aset = set(ans_token)\n",
      "    a_len = len(ans_token)\n",
      "\n",
      "    # q_index = np.arange(1,q_len)\n",
      "    a_index = np.arange(1,a_len + 1)\n",
      "\n",
      "    overlap = qset.intersection(aset)\n",
      "    # for i,q in enumerate(cut(question)[:q_len]):\n",
      "    #     value = 1\n",
      "    #     if q in overlap:\n",
      "    #         value = 2\n",
      "    #     q_index[i] = value\n",
      "    for i,a in enumerate(ans_token):\n",
      "        if a in overlap:\n",
      "            a_index[i] = OVERLAP\n",
      "    return a_index\n",
      "\n",
      "\n",
      "\n",
      "def getBatch48008(df,alphabet,batch_size,sort_by_len = True,shuffle = False):\n",
      "    q,a,neg_a=[],[],[]\n",
      "    answers=df[\"answer\"][:250]\n",
      "    ground_truth=df.groupby(\"question\").apply(lambda group: group[group.flag==1].index[0]%250 ).to_dict() \n",
      "    \n",
      "    for question in tqdm(df['question'].unique()):\n",
      "                   \n",
      "        index= ground_truth[question]  \n",
      "        \n",
      "        canindates = [i for i in range(250)]\n",
      "        canindates.remove(index)\n",
      "        a_neg_index = random.choice(canindates)\n",
      "\n",
      "        seq_q = encode_to_split(question,alphabet)\n",
      "        seq_a = encode_to_split(answers[index],alphabet)\n",
      "        seq_neg_a = encode_to_split(answers[a_neg_index],alphabet)\n",
      "        \n",
      "        q.append(seq_q)       \n",
      "        a.append( seq_a)\n",
      "        neg_a.append(seq_neg_a )\n",
      "        \n",
      "    return iteration_batch(q,a,neg_a,batch_size,sort_by_len,shuffle)    \n",
      "def iteration_batch(q,a,neg_a,batch_size,sort_by_len = True,shuffle = False):\n",
      "\n",
      "\n",
      "    if sort_by_len:\n",
      "        sorted_index = sorted(range(len(q)), key=lambda x: len(q[x]), reverse=True)\n",
      "        q = [ q[i] for i in sorted_index]\n",
      "        a = [a[i] for i in sorted_index]\n",
      "        neg_a = [ neg_a[i] for i in sorted_index]\n",
      "\n",
      "        pos_overlap = [pos_overlap[i] for i in sorted_index]\n",
      "        neg_overlap = [neg_overlap[i] for i in sorted_index]\n",
      "\n",
      "    #get batch\n",
      "    m = 0\n",
      "    n = len(q)\n",
      "\n",
      "    idx_list = np.arange(m,n,batch_size)\n",
      "    if shuffle:\n",
      "        np.random.shuffle(idx_list)\n",
      "\n",
      "    mini_batches = []\n",
      "    for idx in idx_list:\n",
      "        mini_batches.append(np.arange(idx,min(idx + batch_size,n)))\n",
      "\n",
      "    for mini_batch in tqdm(mini_batches):\n",
      "        mb_q = [ q[t] for t in mini_batch]\n",
      "        mb_a = [ a[t] for t in mini_batch]\n",
      "        mb_neg_a = [ neg_a[t] for t in mini_batch]\n",
      "        mb_pos_overlap = [pos_overlap[t] for t in mini_batch]\n",
      "        mb_neg_overlap = [neg_overlap[t] for t in mini_batch]\n",
      "        mb_q,mb_q_mask = prepare_data(mb_q)\n",
      "        mb_a,mb_pos_overlaps = prepare_data(mb_a,mb_pos_overlap)\n",
      "        mb_neg_a,mb_neg_overlaps = prepare_data(mb_neg_a,mb_neg_overlap)\n",
      "        # mb_a,mb_a_mask = prepare_data(mb_a,mb_pos_overlap)\n",
      "\n",
      "        # mb_neg_a , mb_a_neg_mask = prepare_data(mb_neg_a)\n",
      "\n",
      "\n",
      "        yield(mb_q,mb_a,mb_neg_a,mb_q_mask,mb_a_mask,mb_a_neg_mask)\n",
      "\n",
      "\n",
      "def get_mini_batch(df,alphabet,batch_size,sort_by_len = True,shuffle = False,model=None,sess=None):\n",
      "    q = []\n",
      "    a = []\n",
      "    neg_a = []\n",
      "    for question in df['question'].unique():\n",
      "#        group = df[df[\"question\"]==question]\n",
      "#        pos_answers = group[df[\"flag\"] == 1][\"answer\"]\n",
      "#        neg_answers = group[df[\"flag\"] == 0][\"answer\"].reset_index()\n",
      "        group = df[df[\"question\"]==question]\n",
      "        pos_answers = group[group[\"flag\"] == 1][\"answer\"]\n",
      "        neg_answers = group[group[\"flag\"] == 0][\"answer\"]#.reset_index()\n",
      "\n",
      "        for pos in pos_answers:\n",
      "            \n",
      "            if model is not None and sess is not None:\n",
      "                \n",
      "                pos_sent= encode_to_split(pos,alphabet)\n",
      "                q_sent,q_mask= prepare_data([pos_sent])\n",
      "                \n",
      "                neg_sents = [encode_to_split(sent,alphabet) for sent in neg_answers] \n",
      "\n",
      "                a_sent,a_mask= prepare_data(neg_sents)                    \n",
      "  \n",
      "                scores = model.predict(sess,(np.tile(q_sent,(len(neg_answers),1)),a_sent,np.tile(q_mask,(len(neg_answers),1)),a_mask))\n",
      "                neg_index = scores.argmax()\n",
      "          \n",
      "\n",
      "               \n",
      "            else:\n",
      "\n",
      "                if len(neg_answers.index) > 0:\n",
      "                    neg_index = np.random.choice(neg_answers.index)\n",
      "            neg = neg_answers.reset_index().loc[neg_index,][\"answer\"]\n",
      "            seq_q = encode_to_split(question,alphabet)\n",
      "            seq_a = encode_to_split(pos,alphabet)\n",
      "            seq_neg_a = encode_to_split(neg,alphabet)\n",
      "            q.append(seq_q)\n",
      "            a.append(seq_a)\n",
      "            neg_a.append(seq_neg_a)\n",
      "    return iteration_batch(q,a,neg_a,batch_size,sort_by_len,shuffle)\n",
      "    \n",
      "\n",
      "def prepare_data(seqs,overlap = None):\n",
      "\n",
      "    lengths = [len(seq) for seq in seqs]\n",
      "    n_samples = len(seqs)\n",
      "    max_len = np.max(lengths)\n",
      "\n",
      "    x = np.zeros((n_samples,max_len)).astype('int32')\n",
      "    if overlap is not None:\n",
      "        overlap_position = np.zeros((n_samples,max_len)).astype('float')\n",
      "\n",
      "        for idx ,seq in enumerate(seqs):\n",
      "            x[idx,:lengths[idx]] = seq\n",
      "            overlap_position[idx,:lengths[idx]] = overlap[idx]\n",
      "        return x,overlap_position\n",
      "    else:\n",
      "        x_mask = np.zeros((n_samples, max_len)).astype('float')\n",
      "        for idx, seq in enumerate(seqs):\n",
      "            x[idx, :lengths[idx]] = seq\n",
      "            x_mask[idx, :lengths[idx]] = 1.0\n",
      "        # print( x, x_mask)\n",
      "        return x, x_mask\n",
      "\n",
      "# def prepare_data(seqs):\n",
      "#     lengths = [len(seq) for seq in seqs]\n",
      "#     n_samples = len(seqs)\n",
      "#     max_len = np.max(lengths)\n",
      "\n",
      "#     x = np.zeros((n_samples, max_len)).astype('int32')\n",
      "#     x_mask = np.zeros((n_samples, max_len)).astype('float')\n",
      "#     for idx, seq in enumerate(seqs):\n",
      "#         x[idx, :lengths[idx]] = seq\n",
      "#         x_mask[idx, :lengths[idx]] = 1.0\n",
      "#     # print( x, x_mask)\n",
      "#     return x, x_mask\n",
      "    \n",
      "\n",
      "def getLogger():\n",
      "    import sys\n",
      "    import logging\n",
      "    import os\n",
      "    import time\n",
      "    now = int(time.time()) \n",
      "    timeArray = time.localtime(now)\n",
      "    timeStamp = time.strftime(\"%Y%m%d%H%M%S\", timeArray)\n",
      "    log_filename = \"log/\" +time.strftime(\"%Y%m%d\", timeArray)\n",
      "    \n",
      "    program = os.path.basename(sys.argv[0])\n",
      "    logger = logging.getLogger(program) \n",
      "    if not os.path.exists(log_filename):\n",
      "        os.mkdir(log_filename)\n",
      "    logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s',datefmt='%a, %d %b %Y %H:%M:%S',filename=log_filename+'/qa'+timeStamp+'.log',filemode='w')\n",
      "    logging.root.setLevel(level=logging.INFO)\n",
      "    logger.info(\"running %s\" % ' '.join(sys.argv))\n",
      "    \n",
      "    return logger\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "from smart_open import open\n",
    "from datasets import load_dataset\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=\"<AWS_ACCESS_KEY_ID>\",\n",
    "    aws_secret_access_key=\"<AWS_SECRET_ACCESS_KEY>\",\n",
    ")\n",
    "s3 = session.client(\"s3\")\n",
    "\n",
    "def download_contents(files):\n",
    "    for file in files:\n",
    "        s3_url = f\"s3://softwareheritage/content/{file['blob_id']}\"\n",
    "        with open(s3_url, \"rb\", compression=\".gz\", transport_params={\"client\": s3}) as fin:\n",
    "            file[\"content\"] = fin.read().decode(file[\"src_encoding\"])\n",
    "    \n",
    "    return {\"files\": files}\n",
    "\n",
    "ds = load_dataset(\"bigcode/the-stack-v2-train-smol-ids\", split=\"train\", streaming=True)\n",
    "ds = ds.map(lambda row: download_contents(row[\"files\"]))\n",
    "ds_iter = iter(ds)\n",
    "for row in ds_iter:\n",
    "    for file in row[\"files\"]:\n",
    "        print(file[\"content\"])\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'repo_name': 'leenismail/GPA-Calculator-', 'repo_url': 'https://github.com/leenismail/GPA-Calculator-', 'snapshot_id': '0081a3c770569eaf9056139ba5b8ece2b1ce3094', 'revision_id': 'c6f840ae14f5693e287f4f89a7e6cca5de27fa64', 'directory_id': '63e52452676d11d076b9a4cdd06de34a192617e8', 'branch_name': 'refs/heads/master', 'visit_date': datetime.datetime(2022, 10, 2, 20, 10, 50, 860436), 'revision_date': datetime.datetime(2020, 6, 1, 19, 35, 31), 'committer_date': datetime.datetime(2020, 6, 1, 19, 35, 31), 'github_id': 267670565, 'star_events_count': 1, 'fork_events_count': 0, 'gha_license_id': None, 'gha_created_at': None, 'gha_updated_at': None, 'gha_pushed_at': None, 'gha_language': None, 'files': [{'blob_id': '84588cdf9bb1acf48fa7e56147936f1df16afc17', 'path': '/cli/GPA.java', 'content_id': '4517343e08877b30341de187b536d7f803629a2d', 'language': 'Java', 'length_bytes': 1196, 'detected_licenses': [], 'license_type': 'no_license', 'src_encoding': 'UTF-8', 'is_vendor': False, 'is_generated': False, 'alphanum_fraction': 0.6086956262588501, 'alpha_fraction': 0.590300977230072, 'num_lines': 47, 'avg_line_length': 23.446807861328125, 'max_line_length': 76, 'content': '\\r\\nimport java.util.Scanner;\\r\\n\\r\\npublic class GPA {\\r\\n\\r\\npublic static void main(String[] args) {\\r\\n\\r\\n\\tint course;\\r\\n\\tint grade;\\r\\n\\tint CourseSum=0;//Sum of all courses\\r\\n\\tdouble CmltivSum=0;//Cumulative grades sum\\r\\n\\tScanner s = new Scanner(System.in);\\r\\n\\t\\r\\n\\tSystem.out.println(\"Please enter number of semesters:\");\\r\\n\\tint semester = s.nextInt();\\r\\n\\t\\r\\n\\tfor (int j = 0; j < semester; j++) {\\r\\n\\t\\t\\r\\n\\t\\tdouble SmstrSum=0;//Sum of grades in each semester\\r\\n\\t\\tSystem.out.println(\"For semester number \"+(j+1)+\":\");\\r\\n\\t\\tSystem.out.println(\"    Enter the number of passed courses:\");\\r\\n\\t\\tcourse = s.nextInt();\\r\\n\\t\\tCourseSum=CourseSum+course;\\r\\n\\t\\r\\n\\t\\tfor (int i = 0; i <course; i++) {\\r\\n\\t\\t\\t\\r\\n\\t\\t\\tSystem.out.println(\"    Enter grade number\" +(i+1)+ \":\");\\r\\n\\t\\t\\tgrade = s.nextInt();\\r\\n\\t\\t\\r\\n\\t\\t\\tif (grade>=35) {\\r\\n\\t\\t\\tCmltivSum=CmltivSum+grade;\\r\\n\\t\\t\\tSmstrSum=SmstrSum+grade;\\r\\n\\t\\t\\t}\\r\\n\\t\\t\\telse {\\r\\n\\t\\t\\t\\tSystem.out.println(\"    Invalid Input!, Please enter a mark above 35.\");\\r\\n\\t\\t\\t\\ti=i-1;\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t}\\r\\n\\t\\t\\t\\r\\n\\t\\t}\\r\\n\\t\\tdouble totalSem=(((SmstrSum/course)/100)*4);\\r\\n\\t\\r\\n\\t\\tSystem.out.println(\"\\\\n    Semester GPA: \"+totalSem+\"/4\\\\n\");\\r\\n\\t}\\r\\n\\tdouble totalCum=(((CmltivSum/CourseSum)/100)*4);\\r\\n\\tSystem.out.println(\"    Cumulative GPA: \"+totalCum+\"/4\");\\r\\n}\\r\\n}'}], 'num_files': 1}\n"
     ]
    }
   ],
   "source": [
    "for file in ds_iter:\n",
    "    print(file)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
