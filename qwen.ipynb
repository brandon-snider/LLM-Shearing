{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandon/Documents/College/q4-fall-24/cs-229/project/LLM-Shearing/.env_qwen/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight: torch.Size([151936, 896])\n",
      "model.layers.0.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.0.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.0.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.0.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.0.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.0.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.0.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.0.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.0.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.0.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.0.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.0.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.1.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.1.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.1.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.1.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.1.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.1.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.1.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.1.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.1.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.1.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.1.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.1.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.2.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.2.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.2.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.2.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.2.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.2.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.2.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.2.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.2.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.2.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.2.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.2.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.3.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.3.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.3.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.3.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.3.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.3.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.3.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.3.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.3.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.3.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.3.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.3.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.4.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.4.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.4.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.4.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.4.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.4.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.4.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.4.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.4.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.4.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.4.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.4.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.5.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.5.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.5.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.5.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.5.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.5.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.5.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.5.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.5.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.5.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.5.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.5.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.6.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.6.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.6.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.6.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.6.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.6.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.6.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.6.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.6.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.6.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.6.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.6.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.7.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.7.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.7.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.7.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.7.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.7.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.7.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.7.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.7.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.7.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.7.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.7.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.8.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.8.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.8.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.8.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.8.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.8.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.8.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.8.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.8.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.8.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.8.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.8.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.9.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.9.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.9.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.9.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.9.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.9.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.9.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.9.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.9.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.9.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.9.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.9.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.10.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.10.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.10.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.10.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.10.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.10.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.10.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.10.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.10.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.10.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.10.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.10.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.11.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.11.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.11.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.11.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.11.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.11.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.11.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.11.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.11.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.11.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.11.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.11.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.12.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.12.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.12.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.12.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.12.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.12.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.12.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.12.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.12.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.12.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.12.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.12.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.13.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.13.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.13.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.13.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.13.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.13.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.13.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.13.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.13.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.13.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.13.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.13.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.14.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.14.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.14.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.14.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.14.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.14.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.14.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.14.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.14.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.14.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.14.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.14.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.15.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.15.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.15.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.15.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.15.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.15.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.15.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.15.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.15.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.15.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.15.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.15.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.16.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.16.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.16.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.16.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.16.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.16.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.16.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.16.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.16.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.16.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.16.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.16.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.17.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.17.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.17.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.17.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.17.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.17.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.17.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.17.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.17.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.17.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.17.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.17.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.18.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.18.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.18.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.18.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.18.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.18.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.18.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.18.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.18.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.18.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.18.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.18.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.19.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.19.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.19.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.19.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.19.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.19.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.19.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.19.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.19.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.19.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.19.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.19.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.20.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.20.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.20.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.20.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.20.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.20.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.20.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.20.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.20.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.20.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.20.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.20.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.21.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.21.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.21.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.21.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.21.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.21.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.21.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.21.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.21.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.21.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.21.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.21.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.22.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.22.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.22.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.22.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.22.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.22.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.22.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.22.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.22.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.22.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.22.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.22.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.layers.23.self_attn.q_proj.weight: torch.Size([896, 896])\n",
      "model.layers.23.self_attn.q_proj.bias: torch.Size([896])\n",
      "model.layers.23.self_attn.k_proj.weight: torch.Size([128, 896])\n",
      "model.layers.23.self_attn.k_proj.bias: torch.Size([128])\n",
      "model.layers.23.self_attn.v_proj.weight: torch.Size([128, 896])\n",
      "model.layers.23.self_attn.v_proj.bias: torch.Size([128])\n",
      "model.layers.23.self_attn.o_proj.weight: torch.Size([896, 896])\n",
      "model.layers.23.mlp.gate_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.23.mlp.up_proj.weight: torch.Size([4864, 896])\n",
      "model.layers.23.mlp.down_proj.weight: torch.Size([896, 4864])\n",
      "model.layers.23.input_layernorm.weight: torch.Size([896])\n",
      "model.layers.23.post_attention_layernorm.weight: torch.Size([896])\n",
      "model.norm.weight: torch.Size([896])\n",
      "lm_head.weight: torch.Size([151936, 896])\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2ForCausalLM\n",
    "\n",
    "# Load the model with remote code for causal language modeling\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "model = Qwen2ForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Print the state dictionary keys to understand the structure\n",
    "state_dict = model.state_dict()\n",
    "for key, value in state_dict.items():\n",
    "    print(f\"{key}: {value.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm()\n",
      "        (post_attention_layernorm): Qwen2RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandon/Documents/College/q4-fall-24/cs-229/project/LLM-Shearing/.env_qwen/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-0.5B', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "\n",
    "print(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9707   11 1879    0]]\n",
      "(1, 4)\n",
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "tokens_np = tokenizer.encode(\"Hello, world!\", add_special_tokens=True, return_tensors=\"np\")\n",
    "print(tokens_np)\n",
    "print(tokens_np.shape)\n",
    "\n",
    "print(tokenizer.decode(tokens_np.flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandon/Documents/College/q4-fall-24/cs-229/project/LLM-Shearing/.env_qwen/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/Users/brandon/Documents/College/q4-fall-24/cs-229/project/LLM-Shearing/models/qwen-tokenizer/tokenizer_config.json',\n",
       " '/Users/brandon/Documents/College/q4-fall-24/cs-229/project/LLM-Shearing/models/qwen-tokenizer/special_tokens_map.json',\n",
       " '/Users/brandon/Documents/College/q4-fall-24/cs-229/project/LLM-Shearing/models/qwen-tokenizer/vocab.json',\n",
       " '/Users/brandon/Documents/College/q4-fall-24/cs-229/project/LLM-Shearing/models/qwen-tokenizer/merges.txt',\n",
       " '/Users/brandon/Documents/College/q4-fall-24/cs-229/project/LLM-Shearing/models/qwen-tokenizer/added_tokens.json',\n",
       " '/Users/brandon/Documents/College/q4-fall-24/cs-229/project/LLM-Shearing/models/qwen-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "save_path = \"/Users/brandon/Documents/College/q4-fall-24/cs-229/project/LLM-Shearing/models/qwen-tokenizer\"\n",
    "tokenizer.save_pretrained(save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
